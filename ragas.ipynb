{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop om Ragas\n",
    "\n",
    "#### Kilder:\n",
    "- [RAGAS](https://docs.ragas.io/en/stable/) <img src=\"media/ragas.png\" alt=\"Ragas logo\" width=\"25\"/>\n",
    "\n",
    "## Begrepsforklaringer\n",
    "##### RAG\n",
    "RAG står for Retrieval-Augmented Generation og gir generativ kunstig intelligens modeler informasjonsinnhenting muligheter. Dette betyr RAG-en hjelper generative KI ved å gi den tilgang til relevant informasjon som kan hjelpe med å svare på spørsmål fra bruker. \n",
    "Informasjonen blir på forhånd lagret omgjort til LLM-embeddings (store vektorer) og lagret i vektor-databaser. Før den generative KI-en svarer, blir brukerspørsmålet gjort om til en egen LLM-embedding og den embeddingen blir brukt til å sammenligne med de andre vektorene i vektor-databasen. Her blir ofte \"Cosine-similarity\" brukt for å finne vektorene som ligner mest. Deretter blir informasjonen som er mest relevant til spørsmålet, gitt til KI-en.\n",
    "\n",
    "### Hva er ragas?\n",
    "Ragas er et bra dokumentert (!), open-source bibliotek som lar deg evaluere LLM-applikasjoner og RAG-er. Viktig for denne evalueringen er metrics.\n",
    "\n",
    "<img src=\"media/metrics_mindmap.png\" alt=\"Metrics Mindmap\" width=\"500\"/>\n",
    "\n",
    "To typer metrics:\n",
    "\n",
    "##### LLM Based\n",
    "- Bruker LLM til å vurdere. \n",
    "- Non-deterministisk ved at LLM ikke alltid vil returnere det samme resultatet\n",
    "- Likevel vist seg å være mer nøyaktige og nærmere menneskelig evaluering\n",
    "\n",
    "##### Non-LLM Based\n",
    "- Bruker **ikke** LLM til å vurdere\n",
    "- Deterministiske \n",
    "- Bruker tradisjonelle metoder for å evaluere\n",
    "- Mindre nøyaktige sammenlignet med menneskelig evaluering\n",
    "\n",
    "\n",
    "To andre kategorier:\n",
    "##### Single Turn Metrics\n",
    "- Evaluerer basert på én runde med interaksjon mellom bruker og generativ KI\n",
    "\n",
    "##### Multiple Turn Metrics\n",
    "- Evalierer basert på flere runder med interaksjon mellom bruker og generativ KI\n",
    "\n",
    "\n",
    "### Hvordan evaluere med Ragas?\n",
    "For å evaluere hvor god en generativ KI trenger man 3 ting:\n",
    "- Spørsmål\n",
    "- Svar\n",
    "- Referanse/riktig svar\n",
    "\n",
    "Med dette kan man evaluere om svaret KI-en gir stemmer opp mot svaret vi forventer.\n",
    "\n",
    "For å evaluere hvor god en RAG er til å gi riktig informasjon til KI-en trenger man 4 ting:\n",
    "- Spørsmål\n",
    "- Svar\n",
    "- Referanse/riktig svar\n",
    "- Gitt kontekst/informasjon\n",
    "\n",
    "Den siste (gitt kontekst) er viktig når man skal vurdere hvor svikten i system ligger. Hvis det er gitt feil kontekst, er det RAG-en som har mislyktes. Hvis det er riktig kontekst, men feil svar, er det KI-en som har mislyktes. \n",
    "\n",
    "Under skal vi se hvordan vi kan evaluere en RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ragas langchain langchain_openai faiss-cpu pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sette env-variabler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your_key\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Ragas Tutorial\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lese fil og lage vektor-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     34\u001b[0m score_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[1;32m---> 35\u001b[0m vector_retriever \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(splits, \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mas_retriever(\n\u001b[0;32m     36\u001b[0m             search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m             search_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     38\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: k,\n\u001b[0;32m     39\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: score_threshold\n\u001b[0;32m     40\u001b[0m                 }\n\u001b[0;32m     41\u001b[0m         )\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Jon Bergland\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:338\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[0;32m    337\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membeddings  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client:\n",
      "File \u001b[1;32mc:\\Users\\Jon Bergland\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter, MarkdownTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.documents import Document\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, ContextEntityRecall, ContextPrecision, FactualCorrectness\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from ragas.utils import safe_nanmean\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as ps\n",
    "\n",
    "# Setter opp RAG-en\n",
    "\n",
    "# Leser dokumentene vi vil legge inn i RAG-en\n",
    "filepath = 'data/dnd_doc.md'\n",
    "\n",
    "\n",
    "dnd_document: str = \"\"\n",
    "with open(filepath, encoding=\"utf-8\") as f:\n",
    "    dnd_document = f.read()\n",
    "\n",
    "# Gjør teksten om til en Document-klasse fra Langchain\n",
    "dnd_document = [Document(dnd_document)]\n",
    "\n",
    "# Splitter dokumentet med en tekstsplitter fra Langchain\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0) # TextSplitter\n",
    "splits = text_splitter.split_documents(dnd_document)\n",
    "\n",
    "# Lager en vektor-database retriever\n",
    "k = 2\n",
    "score_threshold = 0.7\n",
    "vector_retriever = FAISS.from_documents(splits, OpenAIEmbeddings()).as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs = {\n",
    "                \"k\": k,\n",
    "                \"score_threshold\": score_threshold\n",
    "                }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generere svar til spørsmålene i CSV-filen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "            model= \"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            max_tokens=16384\n",
    "        )\n",
    "\n",
    "def convert_docs_to_strings(docs: list[Document]) -> list[str]:\n",
    "        \"\"\"Convert Objects of Document type to an list of strings\"\"\"\n",
    "        return [doc.page_content for doc in docs]\n",
    "\n",
    "def get_relevant_docs(query: str) -> list[str]:\n",
    "    return convert_docs_to_strings(vector_retriever.invoke(input=query))\n",
    "\n",
    "def generate_answer(query: str, relevant_doc: list[Document]):\n",
    "        \"\"\"Generate an answer for a given query based on the most relevant document.\"\"\"\n",
    "        prompt = f\"question: {query}\\n\\nDocuments: {relevant_doc}\"\n",
    "        messages = [\n",
    "            (\"system\", \"You are a helpful assistant that answers questions based on given documents only.\"),\n",
    "            (\"human\", prompt),\n",
    "        ]\n",
    "        ai_msg = llm.invoke(messages)\n",
    "        return ai_msg.content\n",
    "\n",
    "dataset = []\n",
    "\n",
    "df = ps.read_csv(\"data/sample_questions_dnd.csv\")\n",
    "querys = df[\"question\"].tolist()\n",
    "responses = df[\"answer\"].tolist()\n",
    "\n",
    "for query, reference in zip(querys, responses):\n",
    "\n",
    "    relevant_docs = get_relevant_docs(query=query)\n",
    "    response = generate_answer(query, relevant_docs)\n",
    "    # Legger til i datasettet spørsmålet, kontekst, response fra KI og referansen/riktig svar\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\":query,\n",
    "            \"retrieved_contexts\":relevant_docs,\n",
    "            \"response\":response,\n",
    "            \"reference\":reference\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluere datasettet\n",
    "For å evaluere datasettet kan man bruke mange ulike metrics, alle listet på nettsiden til Ragas (https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/)\n",
    "\n",
    "- Context Precision\n",
    "- Context Recall\n",
    "- Context Entities Recall\n",
    "- Noise Sensitivity\n",
    "- Response Relevancy\n",
    "- Faithfulness\n",
    "- Multimodal Faithfulness\n",
    "- Multimodal Relevance\n",
    "- Factual Correctness\n",
    "- Semantic Similarity\n",
    "- Non LLM String Similarity\n",
    "- BLEU Score\n",
    "- ROUGE Score\n",
    "- String Presence\n",
    "- Exact Match\n",
    "\n",
    "Her kommer vi til å bruke to til å vurdere RAG-en (Context Recall og Context Precision) og en for å vurdere responsen til LLM-en (Factual Correctness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluerer datasettet\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), ContextPrecision(), FactualCorrectness()],\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "\n",
    "context_recall = safe_nanmean(result[\"context_recall\"])\n",
    "context_precision = safe_nanmean(result[\"context_precision\"])\n",
    "factual_correctness = safe_nanmean(result[\"factual_correctness\"])\n",
    "\n",
    "print(context_recall, context_precision, factual_correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste ulike parametere for å finne beste kombinasjon for RAG-en\n",
    "Basert på resultatene kan vi forbedre RAG-en ved å endre på ulike aspekter av RAG-en. Det er to måter måter man kan gjøre det på:\n",
    "\n",
    "#### Splitters\n",
    "\n",
    "Det første aspektet er hvordan vi deler opp dokumentet før vi legger det inn i databasen. Med klassen RecursiveCharacterSplitter kan vi endre på hvor store chunks vi lager og hvor mye overlap de har. Dette bestemmer hvor mye informasjon det er i hver kontekst vi gir LLM-en. \n",
    "\n",
    "Mindre kontekst:\n",
    "- Mindre informasjon til LLM-en per spørring\n",
    "- Kan svekke nøyaktigheten noe, \n",
    "- Krever mindre tokens. \n",
    "\n",
    "Større kontekst:\n",
    "- Mer informasjon til LLM-en\n",
    "- Kan øke nøyaktigheten\n",
    "- Krever mer tokens\n",
    "\n",
    "Man kan også bruke andre splittere (eller lage egne). Her er andre splittere fra Langchain som kan brukes:\n",
    "- CharacterTextSplitter\n",
    "- MarkdownHeaderTextSplitter\n",
    "- RecursiveJsonSplitter\n",
    "- SemanticChunker\n",
    "\n",
    "#### Retrivers (vektordatabase)\n",
    "Hvordan dataen blir lagret og hvordan den blir hentet er mye å si på nøyaktigheten til RAG-en. Med FAISS som vectorstore er det tre parametere man kan endre på:\n",
    "- Search Type\n",
    "   - similarity\n",
    "   - similiarity_score_threshold\n",
    "   - mmr\n",
    "- k\n",
    "    - Maksimalt antall kontekster som blir hentet\n",
    "\n",
    "- score_threshold\n",
    "    - Likhetsscore for at en kontekst skal kunne bli hentet av k\n",
    "\n",
    "Man kan også bruke andre måter å hente dokumenter på. I tillegg til egen implementerte metoder, har Langchain flere ulike retrievers:\n",
    "- ParentDocumentRetriever, \n",
    "- EnsembleRetriever \n",
    "- MultiVectorRetriever\n",
    "- BM25Retriver\n",
    "\n",
    "\n",
    "\n",
    "##### Prøv dere fram og forsøk å få høyest mulig score!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hvis tid\n",
    "## Optuna\n",
    "Et bibliotek for å automatisere testing av parametere. Man spesifiserer en oppgave med ulike parametere (som bygging av en RAG) og sier hvilke verdier som den skal prøve å forbedre (for eksempel Context Recall fra RAGAS). Dette kan gjentas så mange ganger man vil og kan pararalliseres. Til slutt kan resultatene fra utprøvingen vises i Optuna sitt eget dashboard.\n",
    "\n",
    "Se filen **optuna_test.py** for kode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U optuna optuna-dashboard optunahub logging cmaes torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
